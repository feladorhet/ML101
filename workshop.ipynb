{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Welcome to our Linear Regression Workshop!\n","\n","In this notebook, we will first discuss what regression is, and how it mathematically looks like. Then, we will dive deep into the implementation of linear regression using Python and some related libraries. After having a total understanding of the concept, we will solve a real-world problem using the model we generated and evaluate its performance. \n","\n","Throughout the workshop, we will be learning a lot of new concepts, their importance, and how they are used in real-life scenarios. Let's get started!"]},{"cell_type":"markdown","metadata":{},"source":["First, let us understand what \"regression\" and \"linear regression\" mean."]},{"cell_type":"markdown","metadata":{},"source":["## Regression\n","- Regression is a statistical technique that relates a dependent variable (denoted as Y) to one or more independent variables (denoted as X).\n","\n","- Regression models are able to show whether the changes observed in the dependent variable are associated with changes in the independent variables.\n","\n","- There are many different regression models, such as \"linear regression\", \"logistic regression\", \"polynomial regression\", \"ridge regression\", and \"lasso regression\", and many others.\n","\n","- In this notebook, we will focus on linear regression, which models the relationship between one dependent variable and one or more independent variables using a straight line, hence the name \"linear\" regression. "]},{"cell_type":"markdown","metadata":{},"source":["### Linear Regression\n","- Linear regression functions are of the form `Y = wX + b`, where\n","\n","    `Y`: dependent variable vector, what we are trying to guess\n","\n","    `w`: weight vector, the importance of each independent variable. We will refer to weights as `coefficients` in this workshop, to make the code more understandable.\n","\n","    `X`: independent variable vector, what we base our predictions on\n","\n","    `b`: bias, helps with fitting the model better. We will refer to bias as `intercept` in this workshop, again to make the code more understandable.\n","\n","- The purpose of `finding` the most accurate model requires us to find the best parameters `w` and `b` such that our regression line is a perfect fit for our dataset. That is, we try many many many different `w` and `b` values to find the line that helps us the most. But how do we find them?  "]},{"cell_type":"markdown","metadata":{},"source":["### Mean Squared Error (MSE)\n","\n","- MSE is a very common measure of the quality of an estimator. It's used to evaluate the accuracy of a linear regression model by calculating the average of the squares of the errors (difference between actual and predicted results).\n","\n","- Formula:\n","\n","$$\n","\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n","$$\n","\n","- The end goal is to minimize MSE to improve the accuracy of the model."]},{"cell_type":"markdown","metadata":{},"source":["### Gradient Descent\n","\n","- Gradient Descent is an optimization algorithm used to minimize the cost function (MSE in our case), by iteratively adjusting model parameters. The update rule for each parameter $\\theta$ is:\n","\n","$$\n","\\theta := \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n","$$\n","\n","where\n","\n","$$\n","$$\n","\n","- $\\theta$ : the parameter we are trying to optimize (w or b for linear regression)\n","\n","- $\\alpha$ : learning rate, a hyperparameter that determines the step size at each iteration while moving toward a minimum cost function. Small alpha ensures more precise updates, but training will take more time with smaller alpha (since changes will be very minimal, but very accurate).\n","\n","- $J(\\theta)$ : the cost function, MSE in our case.\n","\n","- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ : Gradient of the cost function with respect to the parameter. It measures how much J changes with a small change in $\\theta$. The gradient points in the direction of the steepest increase of the cost function."]},{"cell_type":"markdown","metadata":{},"source":["### Now that we know what linear regression looks like, let us start implementing it using Python!"]},{"cell_type":"markdown","metadata":{},"source":["Let's import the necessary libraries first."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["import numpy as np\n","import sklearn.metrics\n","import joblib"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's implement the Linear Regression class using Python together!"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["class LinearRegression:\n","\n","    def __init__(\n","        self,\n","        method=\"normal_equation\",\n","        learning_rate=0.01,\n","        epochs=1000,\n","        regularization=None,\n","        alpha=0.01,\n","    ):\n","        self.coefficients = None\n","        self.intercept = None\n","        self.method = method\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.regularization = regularization\n","        self.alpha = alpha  # Regularization strength\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","            Fit the model based on the method of choice.\n","        \"\"\"\n","        X = np.array(X)\n","        y = np.array(y)\n","\n","        if self.method == \"normal_equation\":\n","            self._fit_normal_equation(X, y)\n","\n","        elif self.method == \"gradient_descent\":\n","            self._fit_gradient_descent(X, y)\n","\n","        else:\n","            raise ValueError(\"Method must be 'normal_equation' or 'gradient_descent'\")\n","\n","    def _fit_normal_equation(self, X, y):\n","        \"\"\"\n","            Fit model directly according to formula Y = w*X + b where w is weight (coefficients) and b is bias (intercept).\n","            This method does not use iteration like gradient descent, but uses the normal equation.\n","        \"\"\"\n","        X_b = np.c_[(np.ones(X.shape[0], 1)), X]  # X_b is the bias term\n","        if self.regularization == \"ridge\":\n","            L = self.alpha * np.eye(X_b.shape[1])\n","            L[0, 0] = 0  # do not regularize the intercept term\n","            theta_best = np.linalg.inv(X_b.T.dot(X_b) + L).dot(X_b.T).dot(y)\n","        else:\n","            theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n","        self.intercept = theta_best[0]\n","        self.coefficients = theta_best[1:]\n","\n","    def _fit_gradient_descent(self, X, y):\n","        \"\"\"\n","            Fit model using gradient descent\n","        \"\"\"\n","        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # X_b is the bias term\n","        m = len(y)\n","        theta = np.random.randn(X_b.shape[1])  # random initialization of theta\n","\n","        for epoch in range(self.epochs):\n","            gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n","            if self.regularization == \"lasso\":\n","                gradients += self.alpha * np.sign(theta)\n","            elif self.regularization == \"ridge\":\n","                gradients += self.alpha * theta\n","                gradients[0] -= self.alpha * theta[0]   # do not regularize the intercept term\n","            theta -= self.learning_rate * gradients\n","\n","        self.intercept = theta[0]\n","        self.coefficients = theta[1:]\n","\n","    def predict(self, X):\n","        \"\"\"\n","            Calculate Y given the X value using the model.\n","        \"\"\"\n","        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n","        return X_b.dot(np.r_[self.intercept, self.coefficients])\n","\n","    def mean_squared_error(self, y_true, y_pred):\n","        \"\"\"\n","            Calculate mean squared error according to the formula.\n","        \"\"\"\n","        return np.mean((y_true - y_pred) ** 2)\n","\n","    def r2_score(self, y_true, y_pred):\n","        \"\"\"\n","            Determine the proportion of variance in Y that can be explained by X.\n","            In other words, a measure of how well the data fits our model.\n","        \"\"\"\n","        total_variance = np.var(y_true) * len(y_true)\n","        explained_variance = total_variance - np.sum((y_true - y_pred) ** 2)\n","        return explained_variance / total_variance\n","\n","    \"\"\"\n","        Following two functions are for saving the model for later use.\n","        If you want to test your model on other datasets, you can save the model to your local. \n","    \"\"\"\n","    def save_model(self, filepath):\n","        joblib.dump(self, filepath)\n","\n","    def load_model(self, filepath):\n","        return joblib.load(filepath)\n"]},{"cell_type":"markdown","metadata":{},"source":["This class definition for Linear Regression covers everything we have discused in theory, and further implements other metrics for accuracy calculations and optimizations."]},{"cell_type":"markdown","metadata":{},"source":["### Now that we have a fully functioning model in our hands, let's put it into use!\n","\n","We are going to use a very famous dataset called `California Housing`, which is a very famous dataset by Carneige Mellon University. Let's see its desription by loading it first. "]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","\n","california_housing = fetch_california_housing(as_frame=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["print(california_housing.DESCR) # see the official description of our dataset"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
