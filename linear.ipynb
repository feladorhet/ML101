{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Welcome to our Linear Regression Workshop!\n","\n","In this notebook, we will first discuss what regression is, and how it mathematically looks like. Then, we will dive deep into the implementation of linear regression using Python and some related libraries. After having a total understanding of the concept, we will solve a real-world problem using the model we generated and evaluate its performance. \n","\n","Throughout the workshop, we will be learning a lot of new concepts, their importance, and how they are used in real-life scenarios. Let's get started!"]},{"cell_type":"markdown","metadata":{},"source":["First, let us understand what \"regression\" and \"linear regression\" mean."]},{"cell_type":"markdown","metadata":{},"source":["## Regression\n","- Regression is a statistical technique that relates a dependent variable (denoted as Y) to one or more independent variables (denoted as X).\n","\n","- Regression models are able to show whether the changes observed in the dependent variable are associated with changes in the independent variables.\n","\n","- There are many different regression models, such as \"linear regression\", \"logistic regression\", \"polynomial regression\", \"ridge regression\", and \"lasso regression\", and many others.\n","\n","- In this notebook, we will focus on linear regression, which models the relationship between one dependent variable and one or more independent variables using a straight line, hence the name \"linear\" regression. "]},{"cell_type":"markdown","metadata":{},"source":["### Linear Regression\n","- Linear regression functions are of the form `Y = wX + b`, where\n","\n","    `Y`: dependent variable vector, what we are trying to guess\n","\n","    `w`: weight vector, the importance of each independent variable. We will refer to weights as `coefficients` in this workshop, to make the code more understandable.\n","\n","    `X`: independent variable vector, what we base our predictions on\n","\n","    `b`: bias, helps with fitting the model better. We will refer to bias as `intercept` in this workshop, again to make the code more understandable.\n","\n","- The purpose of `finding` the most accurate model requires us to find the best parameters `w` and `b` such that our regression line is a perfect fit for our dataset. That is, we try many many many different `w` and `b` values to find the line that helps us the most. But how do we find them?  "]},{"cell_type":"markdown","metadata":{},"source":["### Mean Squared Error (MSE)\n","\n","- MSE is a very common measure of the quality of an estimator. It's used to evaluate the accuracy of a linear regression model by calculating the average of the squares of the errors (difference between actual and predicted results).\n","\n","- Formula:\n","\n","$$\n","\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n","$$\n","\n","- The end goal is to minimize MSE to improve the accuracy of the model."]},{"cell_type":"markdown","metadata":{},"source":["### Gradient Descent\n","\n","- Gradient Descent is an optimization algorithm used to minimize the cost function (MSE in our case), by iteratively adjusting model parameters. The update rule for each parameter $\\theta$ is:\n","\n","$$\n","\\theta := \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n","$$\n","\n","where\n","\n","$$\n","$$\n","\n","- $\\theta$ : the parameter we are trying to optimize (w or b for linear regression)\n","\n","- $\\alpha$ : learning rate, a hyperparameter that determines the step size at each iteration while moving toward a minimum cost function. Small alpha ensures more precise updates, but training will take more time with smaller alpha (since changes will be very minimal, but very accurate).\n","\n","- $J(\\theta)$ : the cost function, MSE in our case.\n","\n","- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ : Gradient of the cost function with respect to the parameter. It measures how much J changes with a small change in $\\theta$. The gradient points in the direction of the steepest increase of the cost function."]},{"cell_type":"markdown","metadata":{},"source":["### Now that we know what linear regression looks like, let us start implementing it using Python!"]},{"cell_type":"markdown","metadata":{},"source":["Let's import the necessary libraries first."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import joblib"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's implement the Linear Regression class using Python together!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LinearRegression:\n","\n","    def __init__(\n","        self,\n","        method=\"normal_equation\",\n","        learning_rate=0.01,\n","        epochs=1000,\n","        regularization=None,\n","        alpha=0.01,\n","    ):\n","        self.coefficients = None\n","        self.intercept = None\n","        self.method = method\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.regularization = regularization\n","        self.alpha = alpha  # Regularization strength\n","\n","    def fit(self, X, y):\n","        X = np.array(X)\n","        y = np.array(y).reshape(-1, 1)  # Ensure y is a column vector\n","\n","        if self.method == \"normal_equation\":\n","            self._fit_normal_equation(X, y)\n","        elif self.method == \"gradient_descent\":\n","            self._fit_gradient_descent(X, y)\n","        else:\n","            raise ValueError(\"Method must be 'normal_equation' or 'gradient_descent'\")\n","\n","    def _fit_normal_equation(self, X, y):\n","        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n","        if self.regularization == \"ridge\":\n","            L = self.alpha * np.eye(X_b.shape[1])\n","            L[0, 0] = 0  # Do not regularize the intercept term\n","            theta_best = np.linalg.inv(X_b.T.dot(X_b) + L).dot(X_b.T).dot(y)\n","        else:\n","            theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n","        self.intercept = theta_best[0, 0]\n","        self.coefficients = theta_best[1:].flatten()\n","\n","    def _fit_gradient_descent(self, X, y):\n","        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term\n","        m = len(y)\n","        theta = np.random.randn(X_b.shape[1], 1)\n","\n","        for epoch in range(self.epochs):\n","            gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n","            if self.regularization == \"ridge\":\n","                regularization_term = self.alpha * np.r_[np.zeros((1, 1)), theta[1:]]\n","                gradients += 2 * regularization_term\n","            theta -= self.learning_rate * gradients\n","\n","        self.intercept = theta[0, 0]\n","        self.coefficients = theta[1:].flatten()\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Calculate Y given the X value using the model.\n","        \"\"\"\n","        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n","        return X_b.dot(np.r_[self.intercept, self.coefficients])\n","\n","    def mean_squared_error(self, y_true, y_pred):\n","        \"\"\"\n","        Calculate mean squared error according to the formula.\n","        \"\"\"\n","        return np.mean((y_true - y_pred) ** 2)\n","\n","    def r2_score(self, y_true, y_pred):\n","        \"\"\"\n","        Determine the proportion of variance in Y that can be explained by X.\n","        In other words, a measure of how well the data fits our model.\n","        \"\"\"\n","        total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n","        explained_variance = np.sum((y_true - y_pred) ** 2)\n","        return 1 - explained_variance / total_variance\n","\n","    \"\"\"\n","    Following two functions are for saving the model for later use.\n","    If you want to test your model on other datasets, you can save the model to your local.\n","    \"\"\"\n","\n","    def save_model(self, filepath):\n","        joblib.dump(self, filepath)\n","\n","    def load_model(self, filepath):\n","        return joblib.load(filepath)"]},{"cell_type":"markdown","metadata":{},"source":["This class definition for Linear Regression covers everything we have discused in theory, and further implements other metrics for accuracy calculations and optimizations."]},{"cell_type":"markdown","metadata":{},"source":["### Now that we have a fully functioning model in our hands, let's put it into use!\n","\n","We are going to use a very famous dataset called `California Housing`, which is a very famous dataset by Carneige Mellon University. Let's see its desription by loading it first. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","import matplotlib.pyplot as plt\n","\n","\n","california_housing = fetch_california_housing(as_frame=True)\n","california_housing = california_housing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(california_housing.DESCR)  # see the official description of our dataset"]},{"cell_type":"markdown","metadata":{},"source":["Now let us visualize the data to understand it better."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["california_housing = california_housing.frame"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["california_housing.hist(bins=50, figsize=(12, 8))\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["From the above histograms of the different features, we can conclude that:\n","\n","1. Features are distributed on very different scales\n","2. In HouseAge and MedHouseVal columns the values are capped at 50 and 5 respectively.\n","\n","For better accuracy, we should preprocess those features. We can either perform feature engineering or clean those problematic instances."]},{"cell_type":"markdown","metadata":{},"source":["Now we plot the housing value with respect to longitude and latitude i.e based on location."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["california_housing.plot(\n","    kind=\"scatter\",\n","    x=\"Longitude\",\n","    y=\"Latitude\",\n","    c=\"MedHouseVal\",\n","    cmap=\"jet\",\n","    colorbar=True,\n","    legend=True,\n","    sharex=False,\n","    figsize=(10, 7),\n","    s=california_housing[\"Population\"] / 100,\n","    label=\"population\",\n","    alpha=0.7,\n",")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The above plot displays the map of California, with the color map corresponding to house value and the radius of the circles corresponding to the population of the areas. What does this heatmap-ish scatter plot tell us?\n","1. Houses near ocean value more.\n","2. House in high population density area also value more but the effect decreases as we move further away from the ocean.\n","3. And there are some outliers"]},{"cell_type":"markdown","metadata":{},"source":["Now let's see the correlations of other variables with median house value. Scale is from -1 to 1. The closer the value is to 1, the more positively correlated the variable is with the median house value. The closer the value is to -1, the more negatively correlated the variable is with the median house value."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["corr = california_housing.corr()\n","corr[\"MedHouseVal\"].sort_values(ascending=True)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, the median income is the most positively correlated variable with the median house value. On the other hand, latitude and longitude are the least correlated variables with the median house value."]},{"cell_type":"markdown","metadata":{},"source":["Now that we have observed our dataset, let's preprocess it and split it into training and testing sets. Let's first validate data by checking for missing values, and type of data in each column."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Check for NA / NaN values\")\n","print(\"------------------------\")\n","print(california_housing.isna().sum())\n","print(\"------------------------\")\n","print(\"Check the datatype of each column\")\n","print(california_housing.dtypes)"]},{"cell_type":"markdown","metadata":{},"source":["We see that there are no missing values in the dataset, and all columns are numerical. Now let's split the data into training and testing sets."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","scale = StandardScaler()\n","\n","X = california_housing.drop(\"MedHouseVal\", axis=1)   # dropping target column\n","y = california_housing[\"MedHouseVal\"]\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","X_train = scale.fit_transform(X_train)\n","X_test = scale.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Now the fun part, generate an instance of our Linear Regression model and train it on the training data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = LinearRegression(\n","    method=\"gradient_descent\",\n","    learning_rate=0.01,\n","    epochs=2000,\n","    regularization=\"ridge\",\n","    alpha=0.01,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have trained our model, let's evaluate its performance on the testing data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = model.predict(X_test)\n","mse = model.mean_squared_error(y_test, predictions)\n","r2 = model.r2_score(y_test, predictions)\n","print(f\"Mean Squared Error: {mse}\")\n","print(f\"R2 Score: {r2}\")"]},{"cell_type":"markdown","metadata":{},"source":["~57% accuracy is not bad for a simple linear regression model!"]},{"cell_type":"markdown","metadata":{},"source":["Let's see our regression line and the actual data points on the graph."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# draw the regression line by plotting the predicted values against the actual values, and see how well they fit the line\n","plt.scatter(y_test, predictions)\n","plt.xlabel(\"Actual Prices\")\n","plt.ylabel(\"Predicted Prices\")\n","plt.title(\"Actual Prices vs Predicted Prices\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, our model is not perfect, but it is doing a good job at predicting the median house value. This is because our dataset is very complex and has many outliers. We can improve our model by using more advanced regression techniques, such as polynomial regression, ridge regression, or lasso regression. We can also try to preprocess our data more effectively to remove outliers and scale our features. In the next workshop, we will discuss these techniques and how to implement them using Python, and see if we can achieve more accurate predictions."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
