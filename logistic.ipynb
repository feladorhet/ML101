{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Welcome to our Logistic Regression Workshop!\n",
    "\n",
    "In our previous workshop, we discussed what linear regression is, and how it mathematically looks like. Now, we are going to start learning a more complicated regression algorithm, _logistic regression_. We will start by describing what logistic regression is, then give some mathematical intuition of how it works. Then, similar to previous week, we will implement our own logistic regression model and compare its performance to our linear regression model.\n"
   ],
   "id": "99ee946c4805694b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start by understanding what _logistic regression_ is.",
   "id": "a396608481b18f0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- Logistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature. Dichotomous means there are only two possible classes. For example, it can be used for cancer detection problems. It computes the probability of an event occurrence. \n",
    "- It is a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function.\n",
    "- Remember the linear regression equation `Y = B + WX`? We will, in some sense, use this in a *normalized* way.\n",
    "- We use a function called `sigmoid` (aka `logistic function`), which is of the form\n",
    "$$\n",
    "\\text{p} = \\frac{1}{1 + e^{-y}}\n",
    "$$\n",
    "- And we will apply sigmoid function on linear regression to obtain the formula\n",
    "$$\n",
    "\\text{p} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n)}}\n",
    "$$\n",
    "- Dependent variable `y` in logistic regression will follow `Bernoulli Distribution`\n",
    "- We will do estimation through `Maximum Likelihood`"
   ],
   "id": "643d002d0d2d287e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What is the difference?\n",
    "- Linear regression is suitable for `continuous output` cases, like stock or house prices based on several different parameters.\n",
    "- Logistic regression is suitable for constant and discrete outputs, like whether a patient has cancer or not.\n",
    "\n",
    "![graphs](https://images.datacamp.com/image/upload/v1660054820/Regression_charts_b9de7355cf.png)\n",
    "\n",
    "- In our linear regression workshop, we used `Mean Squared Error (MSE)` to calculate the accuracy of our model, which is a `distance approximation` method. Now, we are going to use a `maximization method` called `Maximum Likelihood Estimation (MLE)` that will determine the parameters that are most likely to produce the observed data. Both are used to estimate the parameters of a regression model. MLE assumes a joint probability mass function, while `MSE` doesn't require any stochastic assumptions for minimizing distance."
   ],
   "id": "8c6c193fbb314a2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sigmoid Function\n",
    "- `Sigmoid` is a function that will give our graph an `S` shape, so that we can fit its center to the origin of our graph, and categorize our data points as `YES` or `NO`s.\n",
    "- Sigmoid is of the form $$\n",
    "\\text{f(x)} = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "- Sigmoid function can take any real value and converts it to a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. We generally say the output is `YES` if the result of sigmoid is greater than `0.5`, and `NO` if it is less than `0.5`.\n",
    "\n",
    "- ![sigmoid](https://images.datacamp.com/image/upload/v1660054820/Line_graph_0672f6583d.png)\n"
   ],
   "id": "fc4bae9d3f37448"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### In today's workshop, we will estimate the survival likelihood of the passengers of Titanic based on different parameters like age, sex, socio-economic status etc...\n",
    "- This dataset is one of the most famous datasets to use for classification tasks, as it's well maintained, and easy to understand for beginners.\n",
    "- We will first explore the dataset, and then implement our own logistic regression model from scratch, and see its performance on the dataset."
   ],
   "id": "fdcec685ee927820"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Let's start by first exploring the dataset!",
   "id": "1f5abe6b9be51526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score"
   ],
   "id": "b72930f7c1163460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the dataset and clean it as needed\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "titanic = pd.read_csv(url)\n",
    "\n",
    "# See the column descriptions for dataset\n",
    "\n",
    "titanic.info()\n",
    "\n",
    "# Column descriptions\n",
    "column_descriptions = {\n",
    "    'PassengerId': 'A unique ID for each passenger',\n",
    "    'Survived': 'Survival (0 = No, 1 = Yes)',\n",
    "    'Pclass': 'Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd)',\n",
    "    'Name': 'Name of the passenger',\n",
    "    'Sex': 'Sex of the passenger',\n",
    "    'Age': 'Age of the passenger',\n",
    "    'SibSp': 'Number of siblings/spouses aboard the Titanic',\n",
    "    'Parch': 'Number of parents/children aboard the Titanic',\n",
    "    'Ticket': 'Ticket number',\n",
    "    'Fare': 'Fare paid by the passenger',\n",
    "    'Cabin': 'Cabin number',\n",
    "    'Embarked': 'Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)'\n",
    "}\n",
    "\n",
    "print(\"\\nColumn Descriptions:\")\n",
    "for column, description in column_descriptions.items():\n",
    "    print(f\"{column}: {description}\")"
   ],
   "id": "8c66347da5d31088",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's clear the dataset by handling the missing values.",
   "id": "b13bdf47ded8cebb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fill missing values for 'Age' with the median value\n",
    "titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())\n",
    "\n",
    "# Fill missing values for 'Embarked' with the most frequent value\n",
    "titanic['Embarked'] = titanic['Embarked'].fillna(titanic['Embarked'].mode()[0])\n",
    "\n",
    "# Drop 'Cabin' if it exists, as it has many missing values\n",
    "if 'Cabin' in titanic.columns:\n",
    "    titanic = titanic.drop('Cabin', axis=1)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_enc = LabelEncoder()\n",
    "titanic['Sex'] = label_enc.fit_transform(titanic['Sex'].astype(str))\n",
    "titanic['Embarked'] = label_enc.fit_transform(titanic['Embarked'].astype(str))\n",
    "\n",
    "# Verify the encoding\n",
    "unique_values = titanic['Sex'].unique()\n",
    "print(f\"Unique values in 'Sex' column: {unique_values}\")"
   ],
   "id": "65f281a0e62d0ad8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's see what the dataset looks like!",
   "id": "df6c0fd6f50e1be3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of Age\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(titanic['Age'], bins=30, kde=True)\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Count of Survived and Not Survived\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Survived', data=titanic)\n",
    "plt.title('Count of Survived and Not Survived')\n",
    "plt.xlabel('Survived')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Survival Rate by Passenger Class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Pclass', y='Survived', data=titanic)\n",
    "plt.title('Survival Rate by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()\n",
    "\n",
    "# Survival Rate by Gender\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Sex', y='Survived', data=titanic)\n",
    "plt.title('Survival Rate by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()\n",
    "\n",
    "# Survival Rate by Embarkation Point\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Embarked', y='Survived', data=titanic)\n",
    "plt.title('Survival Rate by Embarkation Point')\n",
    "plt.xlabel('Embarked')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ],
   "id": "2c2f96e9246efe4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see from the graphs; gender and transportation class seems to be highly likely to be related with death in our dataset. Now that we have related observations, let's prepare our dataset for training the model.",
   "id": "4816c2404f2257b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select features and target variable\n",
    "X = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = titanic['Survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "id": "ebfd395c0ec3ba62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Our dataset is ready to be fed into our logistic regression model, now it's time to implement the model itself!",
   "id": "86c53493bfdca0bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, regularization=None, alpha=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        m, n = X_b.shape\n",
    "        theta = np.random.randn(n, 1)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            predictions = self._sigmoid(X_b.dot(theta))\n",
    "            errors = predictions - y\n",
    "            gradients = X_b.T.dot(errors) / m\n",
    "            if self.regularization == \"ridge\":\n",
    "                regularization_term = self.alpha * np.r_[np.zeros((1, 1)), theta[1:]]\n",
    "                gradients += regularization_term / m\n",
    "            theta -= self.learning_rate * gradients\n",
    "\n",
    "        self.intercept = theta[0, 0]\n",
    "        self.coefficients = theta[1:].flatten()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return self._sigmoid(X_b.dot(np.r_[self.intercept, self.coefficients]))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(y_true, y_pred):\n",
    "        return np.mean(y_true == y_pred)\n"
   ],
   "id": "aab102c2528dbe5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's train the model on our dataset:",
   "id": "3946a99c43e63223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LogisticRegression(learning_rate=0.01, epochs=10000, regularization=\"ridge\", alpha=0.01)\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "18dad4b80d4a53ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = model.predict(X_test)\n",
    "proba_predictions = model.predict_proba(X_test)"
   ],
   "id": "d4db7298518b39ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "accuracy = model.accuracy(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ],
   "id": "40648890a0ed3d4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Approximately 80%, not bad at all! Now let's analyze the performance even further.",
   "id": "c1b413787f3bd200"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "class_report = classification_report(y_test, predictions)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ],
   "id": "bc30b29145497e25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding confusion matrix\n",
    "- A confusion matrix is a performance measurement tool for classification problems in machine learning. It is a table that summarizes the performance of a classification model by showing the true and predicted classifications.\n",
    "- For a binary classification problem, this matrix is in 2x2 shape, and rows/columns are labelled as follows:\n",
    "\n",
    "\n",
    "\n",
    "- \n",
    "\n",
    "    |                   | **Predicted Positive** | **Predicted Negative** |\n",
    "    |-------------------|------------------------|------------------------|\n",
    "    | **Actual Positive** | True Positive (TP)    | False Negative (FN)    |\n",
    "    | **Actual Negative** | False Positive (FP)   | True Negative (TN)     |\n",
    "\n",
    "\n",
    "\n",
    "- `True Positive (TP)`: The number of correct predictions that an instance is positive.\n",
    "- `False Negative (FN)`: The number of incorrect predictions that an instance is negative.\n",
    "- `False Positive (FP)`: The number of incorrect predictions that an instance is positive.\n",
    "- `True Negative (TN)`: The number of correct predictions that an instance is negative.\n",
    "\n",
    "- #### Metrics Derived from Confusion Matrix\n",
    "\n",
    "- **Accuracy**: The proportion of the total number of predictions that were correct.\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "  $$\n",
    "  \n",
    "- **Precision**: The proportion of positive predictions that were correct.\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "  \n",
    "- **Recall (Sensitivity or True Positive Rate)**: The proportion of actual positives that were correctly identified.\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "  \n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "  $$\n",
    "  \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "  "
   ],
   "id": "f440b59732f3f8b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Plotting the ROC Curve\n",
    "- A Receiver Operating Characteristic (ROC) curve is a graphical representation used to assess the performance of a binary classification model. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "- `True positive rate (TPR)`:\n",
    "\n",
    "  $$\n",
    "  \\text{TPR} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "\n",
    "- `False positive rate (FPR)`:\n",
    "  \n",
    "  $$\n",
    "  \\text{FPR} = \\frac{FP}{FP + TN}\n",
    "  $$"
   ],
   "id": "f647f98a775b17ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the ROC curve\n",
    "\n",
    "- X-axis (False Positive Rate): Represents the rate of false positives.\n",
    "- Y-axis (True Positive Rate): Represents the rate of true positives.\n",
    "- Diagonal Line (Random Classifier): The line y=xy=x represents a random classifier. A model with no discrimination capability between classes will lie along this line.\n",
    "\n",
    "- Curve Analysis:\n",
    "    - Closer to Top Left Corner: Indicates a better performing model with higher TPR and lower FPR.\n",
    "    - Area Under the Curve (AUC): The AUC value ranges from 0 to 1, where 1 indicates a perfect model and 0.5 indicates a random classifier.\n",
    "\n",
    "- Evaluating model performance based on area under curve `(AUC)`:\n",
    "    - `AUC = 1`: Perfect classifier\n",
    "    - `0.5 < AUC < 1`: Better than random guessing\n",
    "    - `AUC = 0.5`: No discrimination capability, equivalent to random guessing\n",
    "    - `AUC < 0.5`: Worse than random guessing\n",
    "\n",
    "Now let's draw the ROC curve for our model."
   ],
   "id": "3065fc953e75c9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, proba_predictions)\n",
    "roc_auc_scratch = roc_auc_score(y_test, proba_predictions)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_scratch:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (Scratch)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "id": "d4fc9978c6912b6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Our area is approximately `0.88`, indicating a pretty good performance on this dataset.\n",
    "\n",
    "- We have successfully engineered our dataset by first handling the missing/outlying values, and used it to train our logistic regression model that we built from scratch.\n",
    "- We observed its performance formally by using common metrics like F1 score, accuracy rate, and even drew the ROC curve to fully evaluate our model's performance.\n",
    "- Further improvements on this task can be done by removing more features from dataset that does not seem to be related to dying in the crash, or experimenting with different epoch counts."
   ],
   "id": "e2913e29a55a7244"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## THANK YOU SO MUCH FOR ATTENDING!\n",
    "#### This was the end of today's workshop. See you in the following weeks with more interesting and entertaining workshops about Machine Learning!"
   ],
   "id": "86af31636fc6a91"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
